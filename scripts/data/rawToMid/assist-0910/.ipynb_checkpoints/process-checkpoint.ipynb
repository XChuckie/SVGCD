{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from itertools import chain\n",
    "pd.set_option(\"mode.chained_assignment\", None) # ingore warning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数据读取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>order_id</th>\n",
       "      <th>assignment_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>assistment_id</th>\n",
       "      <th>problem_id</th>\n",
       "      <th>original</th>\n",
       "      <th>correct</th>\n",
       "      <th>attempt_count</th>\n",
       "      <th>ms_first_response</th>\n",
       "      <th>...</th>\n",
       "      <th>hint_count</th>\n",
       "      <th>hint_total</th>\n",
       "      <th>overlap_time</th>\n",
       "      <th>template_id</th>\n",
       "      <th>answer_id</th>\n",
       "      <th>answer_text</th>\n",
       "      <th>first_action</th>\n",
       "      <th>bottom_hint</th>\n",
       "      <th>opportunity</th>\n",
       "      <th>opportunity_original</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>33022537</td>\n",
       "      <td>277618</td>\n",
       "      <td>64525</td>\n",
       "      <td>33139</td>\n",
       "      <td>51424</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>32454</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>32454</td>\n",
       "      <td>30799</td>\n",
       "      <td>NaN</td>\n",
       "      <td>26</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>33022709</td>\n",
       "      <td>277618</td>\n",
       "      <td>64525</td>\n",
       "      <td>33150</td>\n",
       "      <td>51435</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4922</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>4922</td>\n",
       "      <td>30799</td>\n",
       "      <td>NaN</td>\n",
       "      <td>55</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>35450204</td>\n",
       "      <td>220674</td>\n",
       "      <td>70363</td>\n",
       "      <td>33159</td>\n",
       "      <td>51444</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>25390</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>42000</td>\n",
       "      <td>30799</td>\n",
       "      <td>NaN</td>\n",
       "      <td>88</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  order_id  assignment_id  user_id  assistment_id  problem_id  \\\n",
       "0           1  33022537         277618    64525          33139       51424   \n",
       "1           2  33022709         277618    64525          33150       51435   \n",
       "2           3  35450204         220674    70363          33159       51444   \n",
       "\n",
       "   original  correct  attempt_count  ms_first_response  ... hint_count  \\\n",
       "0         1        1              1              32454  ...          0   \n",
       "1         1        1              1               4922  ...          0   \n",
       "2         1        0              2              25390  ...          0   \n",
       "\n",
       "  hint_total  overlap_time  template_id  answer_id answer_text  first_action  \\\n",
       "0          3         32454        30799        NaN          26             0   \n",
       "1          3          4922        30799        NaN          55             0   \n",
       "2          3         42000        30799        NaN          88             0   \n",
       "\n",
       "  bottom_hint opportunity  opportunity_original  \n",
       "0         NaN           1                   1.0  \n",
       "1         NaN           2                   2.0  \n",
       "2         NaN           1                   1.0  \n",
       "\n",
       "[3 rows x 31 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rawdatadir = '../../rawdata/'\n",
    "middatadir = '../../middata/'\n",
    "\n",
    "# 读取数据\n",
    "df = pd.read_csv(os.path.join(rawdatadir, 'assist-0910/skill_builder_data_corrected_collapsed.csv'), encoding='latin1', low_memory=False)\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>order_id</th>\n",
       "      <th>stu_id</th>\n",
       "      <th>exer_id</th>\n",
       "      <th>cpt_seq</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20401</th>\n",
       "      <td>20224085</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[89]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20402</th>\n",
       "      <td>20224095</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>[89]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20403</th>\n",
       "      <td>20224113</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>[89]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       order_id  stu_id  exer_id cpt_seq  label\n",
       "20401  20224085       0        0    [89]      0\n",
       "20402  20224095       0        1    [89]      1\n",
       "20403  20224113       0        2    [89]      1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data select\n",
    "data = df.copy()\n",
    "use_cols = ['order_id', 'user_id', 'problem_id', 'skill_id', 'correct']\n",
    "data = data[use_cols]\n",
    "\n",
    "# process data\n",
    "## step1.0: sort data according to order_id\n",
    "data = data.sort_values('order_id')\n",
    "\n",
    "## step1.1: Remove questions without skill\n",
    "data = data.dropna(subset=['skill_id'])\n",
    "\n",
    "# step1.2: 去除重复习题，仅保留第一次做答习题\n",
    "data = data.groupby('user_id').apply(lambda x: x.drop_duplicates(subset='problem_id', keep='first')).reset_index(drop=True)\n",
    "data = data.sort_values(by='order_id')\n",
    "\n",
    "## step1.3: Remove users with answer nums < 5 | 用户交互的不同知识点需要大于5\n",
    "data = data.groupby('user_id').filter(lambda q: len(q) >= 15).copy()\n",
    "origin_data = data.groupby('user_id').filter(lambda q: len(set(list(chain.from_iterable(q['skill_id'])))) >= 5).copy()\n",
    "\n",
    "## 验证部分：辅助长尾现象的认知诊断\n",
    "# origin_data = origin_data.groupby('user_id').filter(lambda q: len(q) < 55).copy()\n",
    "\n",
    "## step1.4: rename operate\n",
    "origin_data = origin_data.rename(columns={'user_id':'stu_id', \n",
    "                                          'problem_id':'exer_id', \n",
    "                                          'skill_id': 'cpt_seq',\n",
    "                                          'correct': 'label'})\n",
    "\n",
    "## step1.4: list cpt_seq generate\n",
    "origin_data['cpt_seq'] = origin_data['cpt_seq'].apply(lambda c: [int(x) for x in c.split('_')])\n",
    "\n",
    "## step1.5: 重编码操作\n",
    "def recodeEK(input):\n",
    "    stumapdic = {}\n",
    "    exermapdic = {}\n",
    "    knowmapdic = {}\n",
    "\n",
    "    # user map\n",
    "    stus = input['stu_id'].unique().tolist()\n",
    "    for index, s in enumerate(stus): stumapdic[s] = index\n",
    "    input['stu_id'] = input['stu_id'].apply(lambda x: stumapdic[x])\n",
    "    \n",
    "    # exercise map\n",
    "    exers = input['exer_id'].unique().tolist()\n",
    "    for index, e in enumerate(exers): exermapdic[e] = index\n",
    "    input['exer_id'] = input['exer_id'].apply(lambda x: exermapdic[x])\n",
    "\n",
    "    # knowledge map\n",
    "    kcInlogs = list(chain.from_iterable(input['cpt_seq']))\n",
    "    unique_kcInlogs = list(set(kcInlogs))\n",
    "    for index, k in enumerate(unique_kcInlogs): knowmapdic[k] = index\n",
    "    input[\"cpt_seq\"] = input[\"cpt_seq\"].apply(lambda x: [knowmapdic[i] for i in x])\n",
    "\n",
    "    return input\n",
    "\n",
    "origin_data = recodeEK(origin_data)\n",
    "\n",
    "origin_data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "origin_data_ = origin_data.copy()\n",
    "origin_data_[\"cpt_seq\"] = [','.join(map(str, seq)) for seq in origin_data_[\"cpt_seq\"]]\n",
    "origin_data_.to_csv(os.path.join(middatadir, 'assist-0910/assist-0910-all.csv'), index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2088\n",
      "17567\n",
      "123\n",
      "256971\n",
      "123.07040229885058\n",
      "1.200458417486798\n",
      "8.199537504651534\n"
     ]
    }
   ],
   "source": [
    "# 统计学生数量\n",
    "print(len(origin_data['stu_id'].unique()))\n",
    "# 统计习题数量\n",
    "print(len(origin_data['exer_id'].unique()))\n",
    "# 统计知识点数量\n",
    "from itertools import chain\n",
    "merged_list = list(chain.from_iterable(origin_data['cpt_seq']))\n",
    "unique_list = list(set(merged_list))\n",
    "print(len(unique_list))\n",
    "# 统计总logs\n",
    "print(len(origin_data))\n",
    "\n",
    "# 统计：每个学生的平均logs\n",
    "print(len(origin_data)/len(origin_data['stu_id'].unique()))\n",
    "# 统计：每个习题的平均概念\n",
    "kcInlogs = list(chain.from_iterable(origin_data['cpt_seq']))\n",
    "print(len(kcInlogs) / len(origin_data))\n",
    "# 统计：每个知识点的平均logs\n",
    "molecule = 0\n",
    "denominator = 0\n",
    "def algPKC(stu):\n",
    "    global molecule, denominator\n",
    "    stu_u = stu[['exer_id', 'cpt_seq']]\n",
    "    stu_u = stu_u.explode('cpt_seq')\n",
    "    molecule = molecule + len(stu_u)\n",
    "    denominator = denominator + len(stu_u['cpt_seq'].unique())\n",
    "    \n",
    "origin_data.groupby('stu_id').apply(algPKC)\n",
    "print(molecule / denominator)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "进行Q矩阵生成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = origin_data.copy()\n",
    "\n",
    "# step 1: 去除相同的习题\n",
    "data = data.drop_duplicates(\"exer_id\")\n",
    "\n",
    "# step 2: 进行Q矩阵生成\n",
    "def transform_Qdata(input_dict):\n",
    "    output_dict = {}\n",
    "\n",
    "    output_dict[\"exer_id:token\"] = input_dict[\"exer_id\"]\n",
    "    # for cpt_seq, convert it to str\n",
    "    output_dict[\"cpt_seq:token_seq\"] = [','.join(map(str, seq)) for seq in input_dict[\"cpt_seq\"]]\n",
    "\n",
    "    return output_dict\n",
    "\n",
    "dataQ_ = transform_Qdata(data[['exer_id', 'cpt_seq']])\n",
    "dataQ = pd.DataFrame(dataQ_)\n",
    "dataQ.head(3)\n",
    "\n",
    "dataQ.to_csv(os.path.join(middatadir, 'assist-0910/assist-0910-Q.csv'), index=False, encoding='utf-8')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "划分方式一：长尾分布场景"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import random\n",
    "\n",
    "random.seed(1234)\n",
    "np.random.seed(1234)\n",
    "\n",
    "data = origin_data.copy()\n",
    "\n",
    "def split_train_test(originD, ratio=0.2):\n",
    "    train = []\n",
    "    test = []\n",
    "    for _, stu_df in originD.groupby('stu_id'):\n",
    "        train_stu = list(stu_df.iterrows())\n",
    "        test_stu = []\n",
    "\n",
    "        testNums = int(len(train_stu) * ratio)  # 测试集数量，用户角度\n",
    "        dataIndices = random.sample(range(len(train_stu)), testNums)  # 获取测试集数据索引Indexs\n",
    "        for idx in sorted(dataIndices, reverse=True):    # 逆序方式从train中pop数据，保证train数据相对索引不会发生变化\n",
    "            test_stu.append(train_stu.pop(idx))\n",
    "\n",
    "        train += [x[1] for x in train_stu]\n",
    "        test += [x[1] for x in test_stu]\n",
    "\n",
    "    return pd.DataFrame(train), pd.DataFrame(test)\n",
    "\n",
    "assis0910train, assis0910test = split_train_test(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "划分方式二：弱覆盖场景 | 方式(4)划分：用户角度，非完成cpt | 后面不会继续做"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from collections import defaultdict\n",
    "# import random\n",
    "\n",
    "# random.seed(1234)\n",
    "# np.random.seed(1234)\n",
    "\n",
    "# data = origin_data.copy()\n",
    "\n",
    "# # split train:test  |  保证weak concept covery\n",
    "# def split_train_test(oDAta, t_ratio=0.2):\n",
    "#     train = []\n",
    "#     test = []\n",
    "#     num = 0\n",
    "\n",
    "#     # 从学生角度, 划分数据为train set和test set\n",
    "#     for _, stu_df in oDAta.groupby('stu_id'):\n",
    "        \n",
    "#         train_stu = list(stu_df.iterrows())  \n",
    "#         test_stu = []\n",
    "#         drop_exers = []  # 划分到测试集中习题对应的idx\n",
    "\n",
    "#         # 统计知识点对应的习题数量，其中知识点作为key，而value是习题的编号  |  对于一题多知识点情况，只统计第一次习题\n",
    "#         cpt_dict = defaultdict(list)\n",
    "#         for e_idx, (_, records) in enumerate(train_stu):\n",
    "#             cpt_dict[records['cpt_seq'][0]].append(e_idx)\n",
    "\n",
    "#         # 将知识点对应的20%习题丢进测试集中\n",
    "#         for _, e_idxs in  cpt_dict.items():\n",
    "#             tmp_exers =  random.sample(e_idxs, int(len(e_idxs) * t_ratio))\n",
    "#             drop_exers.extend(tmp_exers)\n",
    "#         gap_num = int(len(train_stu) * t_ratio) - len(drop_exers)\n",
    "#         if gap_num:\n",
    "#             trian_exers = [e_idx for e_idx in range(len(train_stu)) if e_idx not in drop_exers]\n",
    "#             drop_exers.extend(random.sample(trian_exers, gap_num))\n",
    "        \n",
    "#         # 将exer_id存在于drop_exers中的数据提取到test中 | 原理：索引号\n",
    "#         for idx in  sorted(drop_exers, reverse=True):\n",
    "#             test_stu.append(train_stu.pop(idx))\n",
    "\n",
    "#         train += [x[1] for x in train_stu]\n",
    "#         test += [x[1] for x in test_stu]\n",
    "\n",
    "#     return pd.DataFrame(train), pd.DataFrame(test)\n",
    "\n",
    "# assis0910train, assis0910test = split_train_test(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "划分方式二：弱覆盖场景 | 方式(3)划分：用户角度，完成cpt划分，排序 | 后面不会继续做"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from collections import defaultdict\n",
    "# import random\n",
    "\n",
    "# random.seed(1234)\n",
    "# np.random.seed(1234)\n",
    "\n",
    "# data = origin_data.copy()\n",
    "\n",
    "# # split train:test  |  保证weak concept covery\n",
    "# def split_train_test(oDAta, t_ratio=0.2):\n",
    "#     train = []\n",
    "#     test = []\n",
    "\n",
    "#     # 从学生角度, 划分数据为train set和test set\n",
    "#     for _, stu_df in oDAta.groupby('stu_id'):\n",
    "        \n",
    "#         train_stu = list(stu_df.iterrows())  \n",
    "#         test_stu = []\n",
    "\n",
    "#         while len(test_stu) < len(stu_df) * t_ratio:\n",
    "#             # 统计知识点对应的习题数量，其中知识点作为key，而value是习题的编号\n",
    "#             cpt_dict = defaultdict(list)\n",
    "#             for e_idx, (_, records) in enumerate(train_stu):\n",
    "#                 for cpt in records['cpt_seq']:\n",
    "#                     cpt_dict[cpt].append(e_idx)\n",
    "#             # 依据知识点对应的习题数量对知识点进行排序\n",
    "#             cpt_sorted = sorted(cpt_dict.items(), key=lambda x: len(x[1]))\n",
    "#             # 将最少数量的知识点对应的所有习题划分测试集\n",
    "#             cpt, indices = cpt_sorted[0]\n",
    "#             if (len(indices) + len(test_stu)) > len(stu_df) * t_ratio:  # 跳出条件\n",
    "#                 break\n",
    "#             for idx in sorted(indices, reverse=True):  # 将exer_id存在于drop_exers中的数据提取到test中 | 原理：索引号\n",
    "#                 test_stu.append(train_stu.pop(idx))\n",
    "#         train += [x[1] for x in train_stu]\n",
    "#         test += [x[1] for x in test_stu]\n",
    "\n",
    "#     return pd.DataFrame(train), pd.DataFrame(test)\n",
    "\n",
    "# assis0910train, assis0910test = split_train_test(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数据分析模块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.296455938697317\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "# data = origin_data.copy() \n",
    "# data = assis0910train.copy() \n",
    "data = assis0910test.copy() \n",
    "\n",
    "\n",
    "## 统计每个学生交互的知识点数量和习题数量、每个习题交互的学生数量\n",
    "data = data[['stu_id', 'exer_id', 'cpt_seq']]\n",
    "data = pd.DataFrame({\n",
    "    'stu_id': data['stu_id'].repeat(data['cpt_seq'].apply(len)),\n",
    "    'exer_id': data['exer_id'].repeat(data['cpt_seq'].apply(len)),\n",
    "    'cpt_id': data['cpt_seq'].explode()\n",
    "})\n",
    "# data = data.drop_duplicates(['stu_id', 'cpt_id']).reset_index(drop=True)\n",
    "\n",
    "# # 统计习题所交互的学生数量\n",
    "# exer_id = []\n",
    "# inter_stuNum = []\n",
    "\n",
    "# for _, stu_df in data.groupby('exer_id'):\n",
    "#     exer_id.extend(stu_df['exer_id'].unique())\n",
    "#     inter_stuNum.append(len(stu_df['stu_id'].unique()))\n",
    "# sorted_data = sorted(zip(exer_id, inter_stuNum), key=lambda x:x[1], reverse=True)\n",
    "# St_exer, St_stuNums = zip(*sorted_data)\n",
    "\n",
    "# 统计学生所交互的习题数量和知识点数量\n",
    "stu_id = []\n",
    "inter_eNum = []\n",
    "inter_cptNum = []\n",
    "\n",
    "for _, stu_df in data.groupby('stu_id'):\n",
    "    stu_id.extend(stu_df['stu_id'].unique())\n",
    "    inter_eNum.append(len(stu_df['exer_id']))  # 用户交互的习题是独一无二的 | 不会出现重复的情景\n",
    "    inter_cptNum.append(len(stu_df['cpt_id'].unique()))  # 用户交互的知识点存在重复的情况 | 不同的习题考察同一知识点\n",
    "\n",
    "# 习题数量(知识点数量:[2])从大到小进行排序，学生编号对应变化\n",
    "sorted_data = sorted(zip(stu_id, inter_eNum, inter_cptNum), key=lambda x:x[1], reverse=True)\n",
    "St_Stu, St_eNums, St_cptNums = zip(*sorted_data)\n",
    "\n",
    "# 统计：每个学生的平均交互知识点\n",
    "print(sum(inter_cptNum) / len(origin_data['stu_id'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "eNumGroup = []  # 存放每组用户交互的习题数量 | 交互习题排序，前700学生作为Head学生\n",
    "\n",
    "for i in range(35):\n",
    "    start = i * 100\n",
    "    end = (i + 1) * 100\n",
    "    eNumGroup.append(sum(St_eNums[start : end]) / sum(St_eNums))\n",
    "    if end == 3500:\n",
    "        eNumGroup.append(sum(St_eNums[end:]) / sum(St_eNums))\n",
    "        print(len(St_eNums[end:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cNumGroup = []\n",
    "\n",
    "sorted_cptNum = sorted(St_cptNums, reverse=True)\n",
    "for i in range(35):\n",
    "    start = i * 100\n",
    "    end = (i + 1) * 100\n",
    "    cNumGroup.append(sum(sorted_cptNum[start : end]) / sum(sorted_cptNum))\n",
    "    if end == 3500:\n",
    "        cNumGroup.append(sum(sorted_cptNum[end:]) / sum(sorted_cptNum))\n",
    "        print(len(sorted_cptNum[end:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6883065578087965\n",
      "0.23706896551724138\n",
      "----------------------------------------------------------------------------------------------------\n",
      "********************************************************************************************************************************************************************************************************\n"
     ]
    }
   ],
   "source": [
    "exerK = 495  # 对应交互习题数量为50,即小于50的都是尾部用户\n",
    "s\n",
    "print(sum(St_eNums[:exerK]) / sum(St_eNums))\n",
    "print(exerK / 2088)\n",
    "print(\"-\" * 100)\n",
    "print(St_eNums[:exerK])\n",
    "\n",
    "print(\"**\" * 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min(St_eNums)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cptK = 737  # 区分头部学生和尾部学生\n",
    "\n",
    "print(sum(sorted_cptNum[:cptK]) / sum(sorted_cptNum))\n",
    "print(cptK / 3644)\n",
    "print(\"-\" * 100)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headCptNum, tailCptNum = St_cptNums[:702], St_cptNums[702:]\n",
    "headExerNum, tailExerNum = St_eNums[:702], St_eNums[702:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HHIdx, HTIdx, THIdx, TTIdx = [], [], [], []\n",
    "\n",
    "for stu, eNum, cNum in zip(St_Stu, St_eNums, St_cptNums):\n",
    "    if eNum > 98 and cNum > 13: HHIdx.append(stu)\n",
    "    elif eNum > 98 and cNum <= 13:  HTIdx.append(stu)\n",
    "    elif eNum <= 98 and cNum > 13:  THIdx.append(stu)\n",
    "    else: TTIdx.append(stu)\n",
    "\n",
    "print(f\"{len(HHIdx)}  {len(HTIdx)}  {len(THIdx)}  {len(TTIdx)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(HHIdx) + len(HTIdx) + len(THIdx) + len(TTIdx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [torch.tensor(0), torch.tensor(1), torch.tensor(3)]\n",
    "b = {}\n",
    "\n",
    "for idx, item in enumerate(a):\n",
    "    b[item.item()] = idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "htIdx = [idx for idx, i in enumerate(headCptNum) if i > 13]\n",
    "\n",
    "selected_elements = [headExerNum[i] for i in htIdx]\n",
    "len(selected_elements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 统计学生关联知识点情况\n",
    "plt.figure(figsize=(32, 12))\n",
    "# 绘制习题\n",
    "plt.subplot(121)\n",
    "bars = plt.bar(np.arange(len(eNumGroup)), eNumGroup, width=0.8, label='ExerNums')  # tick_label是设置横坐标标签\n",
    "## 绘制知识点\n",
    "plt.subplot(122)\n",
    "bars = plt.bar(np.arange(len(cNumGroup)), cNumGroup, width=0.8, label='CptNums')  # tick_label是设置横坐标标签\n",
    "\n",
    "# # 绘制折线图\n",
    "# plots = plt.plot(np.arange(len(St_cptNums)), [number * 10 for number in St_cptNums], color=\"red\", label='CptNums')\n",
    "\n",
    "# plt.xlabel('Students')\n",
    "# plt.ylabel('Nums')\n",
    "\n",
    "# plt.axhline(y=55, color='red', linestyle='--', label='Threshold')\n",
    "# plt.text(x=3400, y=59, s=r'$y=55$', fontsize=15, color='blue')\n",
    "\n",
    "# 显示条形图\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "训练集和测试集存储"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## assis0910train to save\n",
    "assis0910train_ = assis0910train.copy()\n",
    "# step 1.1: sort train data according to order_id\n",
    "assis0910train_ = assis0910train_.sort_values('order_id')\n",
    "# step 1.2: save user info\n",
    "assis0910train_ = assis0910train_[['stu_id', 'exer_id', 'label', 'cpt_seq']].rename(columns={'stu_id':'stu_id:token', \n",
    "                                                                                  'exer_id':'exer_id:token',\n",
    "                                                                                  'label':'label:float',})\n",
    "\n",
    "assis0910train_[\"cpt_seq\"] = [','.join(map(str, seq)) for seq in assis0910train_[\"cpt_seq\"]]\n",
    "assis0910train_.to_csv(os.path.join(middatadir, 'assist-0910/assist-0910-train.inter.csv'), index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## assis0910test to save\n",
    "assis0910test_ = assis0910test.copy()\n",
    "# step 1.1: sort train data according to order_id\n",
    "assis0910test_ = assis0910test_.sort_values('order_id')\n",
    "# step 1.2: save user info\n",
    "assis0910test_ = assis0910test_[['stu_id', 'exer_id', 'label', 'cpt_seq']].rename(columns={'stu_id':'stu_id:token', \n",
    "                                                                                'exer_id':'exer_id:token',\n",
    "                                                                                'label':'label:float',})\n",
    "\n",
    "assis0910test_[\"cpt_seq\"] = [','.join(map(str, seq)) for seq in assis0910test_[\"cpt_seq\"]]\n",
    "assis0910test_.to_csv(os.path.join(middatadir, 'assist-0910/assist-0910-test.inter.csv'), index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(assis0910train))\n",
    "print(len(assis0910test))\n",
    "print(len(assis0910train) / (len(assis0910test) + len(assis0910train)))\n",
    "print(len(assis0910test) / (len(assis0910test) + len(assis0910train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch1.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "716fc6e9d7bd3edcec119dc168be372f5cefc65262ad4fb0d655d9c015d682aa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
